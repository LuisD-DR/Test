# -*- coding: utf-8 -*-
"""Untitled16.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cCupjmgvbNa79OxUakMlBHYUTwrBiuGS
"""

# app_streamlit.py
"""
App √∫nica que integra:
 - Carga del dataset Iris
 - EDA: histogramas, scatter matrix
 - Preprocesamiento (LabelEncoder, StandardScaler)
 - Entrenamiento RandomForest (bot√≥n para entrenar/actualizar)
 - M√©tricas (accuracy, precision, recall, f1, matriz confusi√≥n, reporte)
 - Guardado / carga de modelo con joblib
 - Dashboard estilo "Panel Universitario" (KPIs, pesta√±as)
 - Panel de predicci√≥n con scatter 3D que sit√∫a la nueva muestra
"""

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import os
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report

# ---------------------------
# Configuraci√≥n de la p√°gina
# ---------------------------
st.set_page_config(page_title="Iris Classification - Dashboard", layout="wide")
st.title("üå∏ Iris Species Classification")
st.caption("Proyecto integrado: EDA, entrenamiento y dashboard (√∫nico archivo).")

# ---------------------------
# Rutas posibles para el CSV y modelo
# ---------------------------
try:
    import kagglehub
    ruta = kagglehub.dataset_download("uciml/iris")
    csv_path = os.path.join(ruta, "Iris.csv")

except Exception as e:
    csv_path = "/content/Iris.csv"


# Cargar el CSV
try:
    df = pd.read_csv(csv_path)
    st.write(df.head())
except Exception as e:
    st.error(f"Error cargando el dataset: {e}")
# ---------------------------
# Funciones utilitarias
# ---------------------------
@st.cache_data
def load_csv_try(paths=CSV_PATHS):
    for p in paths:
        if os.path.exists(p):
            df = pd.read_csv(p)
            return df, p
    return None, None

@st.cache_resource
def load_model_if_exists(path=MODEL_PATH):
    if os.path.exists(path):
        try:
            saved = joblib.load(path)
            return saved
        except Exception as e:
            return None
    return None

def rename_iris_cols(df):
    # Protege si el CSV tiene cabeceras diferentes
    df = df.copy()
    # Trim columns
    df.columns = [c.strip() for c in df.columns]
    # If it's the Kaggle file, first col might be Id
    if len(df.columns) >= 5:
        mapping = {}
        # attempt to map by heuristics
        colnames = df.columns.tolist()
        # If column names are standard use them, otherwise rename by position
        try:
            # common Kaggle Iris.csv has columns: Id, SepalLengthCm, SepalWidthCm, PetalLengthCm, PetalWidthCm, Species
            if "Species" in colnames or "species" in [c.lower() for c in colnames]:
                # find indices
                # build mapping trusting positions (works for both provided code examples)
                mapping[colnames[1]] = "sepal_length"
                mapping[colnames[2]] = "sepal_width"
                mapping[colnames[3]] = "petal_length"
                mapping[colnames[4]] = "petal_width"
                mapping[colnames[5]] = "species"
                df = df.rename(columns=mapping)
        except Exception:
            # fallback: use first 5 numeric-like columns
            if len(colnames) >= 5:
                df = df.rename(columns={
                    colnames[0]: "id",
                    colnames[1]: "sepal_length",
                    colnames[2]: "sepal_width",
                    colnames[3]: "petal_length",
                    colnames[4]: "petal_width",
                })
                # last column to species if exists
                if len(colnames) > 5:
                    df = df.rename(columns={colnames[5]: "species"})
    # lower-case species column name if needed
    if "Species" in df.columns:
        df = df.rename(columns={"Species": "species"})
    return df

def prepare_Xy(df):
    features = ["sepal_length","sepal_width","petal_length","petal_width"]
    for f in features:
        if f not in df.columns:
            raise ValueError(f"Falta la columna {f} en el dataset.")
    X = df[features].values
    y = df["species"].values
    return X, y

def train_and_save_model(X, y, model_path=MODEL_PATH, random_state=42):
    le = LabelEncoder()
    y_enc = le.fit_transform(y)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y_enc, test_size=0.2, random_state=random_state, stratify=y_enc)
    scaler = StandardScaler()
    X_train_s = scaler.fit_transform(X_train)
    X_test_s = scaler.transform(X_test)

    rf = RandomForestClassifier(n_estimators=100, random_state=random_state)
    rf.fit(X_train_s, y_train)

    y_pred = rf.predict(X_test_s)
    metrics = {
        "accuracy": accuracy_score(y_test, y_pred),
        "precision": precision_score(y_test, y_pred, average="weighted", zero_division=0),
        "recall": recall_score(y_test, y_pred, average="weighted", zero_division=0),
        "f1": f1_score(y_test, y_pred, average="weighted", zero_division=0),
        "confusion_matrix": confusion_matrix(y_test, y_pred),
        "classification_report": classification_report(y_test, y_pred, zero_division=0, output_dict=False),
        "le": le,
        "scaler": scaler,
        "model": rf
    }

    # Save combined
    joblib.dump({
        "model": rf,
        "scaler": scaler,
        "labelencoder": le
    }, model_path)

    return metrics

def predict_sample(model_bundle, sample):
    scaler = model_bundle["scaler"]
    rf = model_bundle["model"]
    le = model_bundle["labelencoder"]
    s = scaler.transform(np.array(sample).reshape(1, -1))
    pred_enc = rf.predict(s)[0]
    pred_label = le.inverse_transform([pred_enc])[0]
    return pred_label

# ---------------------------
# Carga del dataset
# ---------------------------
df_raw, loaded_path = load_csv_try()
if df_raw is None:
    st.error("No se encontr√≥ Iris.csv en las rutas conocidas. Sube el archivo 'Iris.csv' en el mismo directorio o usa el uploader abajo.")
    uploaded_file = st.file_uploader("Sube Iris.csv (o arrastra aqu√≠)", type=["csv"])
    if uploaded_file is None:
        st.stop()
    else:
        df_raw = pd.read_csv(uploaded_file)
        loaded_path = "uploaded"

# Normalizar nombres columnas y preparar df
try:
    df = rename_iris_cols(df_raw)
except Exception as e:
    st.error(f"Error renombrando columnas: {e}")
    st.stop()

# Verify required cols
required_cols = ["sepal_length","sepal_width","petal_length","petal_width","species"]
if not all(c in df.columns for c in required_cols):
    st.error(f"El dataset debe contener las columnas: {required_cols}. Columnas encontradas: {df.columns.tolist()}")
    st.stop()

# ---------------------------
# Sidebar: filtros y acciones
# ---------------------------
st.sidebar.header("Controles")
st.sidebar.write(f"Dataset cargado desde: **{loaded_path}**")
if os.path.exists(PDF_PATH):
    st.sidebar.markdown(f"[Ver enunciado PDF]({PDF_PATH})")
else:
    st.sidebar.info("PDF del enunciado no subido en la ruta esperada.")

# seleccion de a√±os ficticios no aplica; usamos filtros por especie si se desea
especies = sorted(df["species"].unique().tolist())
sel_especies = st.sidebar.multiselect("Seleccionar especie(s) para filtrar visualizaciones", options=especies, default=especies)

# Entrenamiento / modelo
model_bundle = load_model_if_exists()
st.sidebar.markdown("---")
train_btn = st.sidebar.button("Entrenar / Actualizar modelo")
retrain_confirm = None

# ---------------------------
# Filtrado de df para visualizaciones
# ---------------------------
df_filtrado = df[df["species"].isin(sel_especies)].copy()

# ---------------------------
# KPIs (se muestran arriba)
# ---------------------------
st.header("üìä Indicadores clave del dataset")
col1, col2, col3 = st.columns(3)

total_samples = int(df_filtrado.shape[0])
unique_species = df_filtrado["species"].nunique()
avg_sepal_length = df_filtrado["sepal_length"].mean()

col1.metric("Muestras (filtradas)", f"{total_samples}")
col2.metric("Especies √∫nicas", f"{unique_species}")
col3.metric("Promedio Sepal Length", f"{avg_sepal_length:.2f}")

st.markdown("---")

# ---------------------------
# Pesta√±as
# ---------------------------
tab1, tab2, tab3 = st.tabs(["üìà Exploraci√≥n", "üß† Modelado", "üîÆ Predicci√≥n"])

# --- TAB 1: Exploraci√≥n ---
with tab1:
    st.subheader("Visualizaciones para understanding + EDA")
    # Histogramas por variable
    st.markdown("**Histogramas por variable (por especie)**")
    df_melt = df_filtrado.melt(id_vars="species", value_vars=["sepal_length","sepal_width","petal_length","petal_width"], var_name="variable", value_name="value")
    fig_hist = px.histogram(df_melt, x="value", color="species", facet_col="variable", facet_col_wrap=2, title="Histogramas por variable", marginal="box")
    st.plotly_chart(fig_hist, use_container_width=True)

    st.markdown("**Scatter Matrix (Pairplot)**")
    fig_matrix = px.scatter_matrix(df_filtrado,
                                   dimensions=["sepal_length","sepal_width","petal_length","petal_width"],
                                   color="species",
                                   title="Scatter Matrix (Pairplot)")
    st.plotly_chart(fig_matrix, use_container_width=True)

    st.markdown("**Scatter 2D ejemplo: Sepal vs Petal**")
    fig2 = px.scatter(df_filtrado, x="sepal_length", y="petal_length", color="species", size="sepal_width", title="Sepal Length vs Petal Length")
    st.plotly_chart(fig2, use_container_width=True)

# --- TAB 2: Modelado ---
with tab2:
    st.subheader("Entrenamiento y m√©tricas del modelo")

    st.markdown("**Estado del modelo**")
    if model_bundle:
        st.success("Modelo encontrado en disco.")
        model_info_cols = st.columns(3)
        # Show basic metrics by loading and running a quick eval if possible
        try:
            le_loaded = model_bundle["labelencoder"]
            scaler_loaded = model_bundle["scaler"]
            rf_loaded = model_bundle["model"]
            model_info_cols[0].write(f"Modelo: RandomForestClassifier (guardado)")
            model_info_cols[1].write(f"LabelEncoder clases: {le_loaded.classes_.tolist()}")
            model_info_cols[2].write(f"Archivo: {MODEL_PATH}")
        except Exception:
            st.warning("No se pudieron leer los metadatos del modelo guardado.")
    else:
        st.info("No se encontr√≥ un modelo guardado. Usa 'Entrenar / Actualizar modelo' para entrenar uno en este archivo.")

    st.markdown("---")

    # Entrenamiento (trigger por bot√≥n)
    if train_btn:
        st.info("Entrenando modelo... esto puede tardar unos segundos.")
        with st.spinner("Entrenando Random Forest y calculando m√©tricas..."):
            try:
                X, y = prepare_Xy(df)
                metrics = train_and_save_model(X, y)
                # show metrics
                st.success("Entrenamiento completado y modelo guardado.")
                st.metric("Accuracy", f"{metrics['accuracy']:.4f}")
                st.metric("Precision (weighted)", f"{metrics['precision']:.4f}")
                st.metric("Recall (weighted)", f"{metrics['recall']:.4f}")
                st.metric("F1-score (weighted)", f"{metrics['f1']:.4f}")

                # Confusion matrix
                cm = metrics["confusion_matrix"]
                fig_cm = go.Figure(data=go.Heatmap(z=cm, x=metrics["le"].classes_, y=metrics["le"].classes_, colorscale="Viridis"))
                fig_cm.update_layout(title="Matriz de confusi√≥n (test set)", xaxis_title="Predicci√≥n", yaxis_title="Verdadero")
                st.plotly_chart(fig_cm, use_container_width=True)

                st.markdown("**Reporte de clasificaci√≥n (texto)**")
                st.text(metrics["classification_report"])

                # reload model_bundle
                model_bundle = load_model_if_exists()
            except Exception as e:
                st.error(f"Error durante el entrenamiento: {e}")

    st.markdown("**Entrenamiento local (explicaci√≥n)**")
    st.write("El entrenamiento utiliza LabelEncoder, StandardScaler y RandomForestClassifier. El dataset se divide en train/test (80/20) estratificado.")

# --- TAB 3: Predicci√≥n ---
with tab3:
    st.subheader("Panel de predicci√≥n y visualizaci√≥n 3D")

    st.markdown("Introduce las medidas de la flor y pulsa 'Predecir' para ver la especie estimada y la posici√≥n de la muestra en un scatter 3D respecto al dataset.")

    # Inputs
    col_a, col_b, col_c, col_d = st.columns(4)
    with col_a:
        sep_len = st.number_input("Sepal length", value=float(df["sepal_length"].median()))
    with col_b:
        sep_wid = st.number_input("Sepal width", value=float(df["sepal_width"].median()))
    with col_c:
        pet_len = st.number_input("Petal length", value=float(df["petal_length"].median()))
    with col_d:
        pet_wid = st.number_input("Petal width", value=float(df["petal_width"].median()))

    pred_button = st.button("Predecir especie")

    # Check model availability
    if model_bundle is None:
        st.warning("No hay modelo cargado. Entrena uno pulsando 'Entrenar / Actualizar modelo' en la barra lateral.")
    else:
        if pred_button:
            try:
                sample = [sep_len, sep_wid, pet_len, pet_wid]
                pred_label = predict_sample(model_bundle, sample)
                st.success(f"Especie predicha: **{pred_label}**")
            except Exception as e:
                st.error(f"Error al predecir: {e}")

        # 3D scatter: ubicamos las dimensiones sepal_length, petal_length, petal_width (ejemplo)
        st.markdown("**Scatter 3D** ‚Äî Sepal length (x), Petal length (y), Petal width (z)")
        fig3d = px.scatter_3d(df_filtrado, x="sepal_length", y="petal_length", z="petal_width", color="species", symbol="species",
                              hover_data=["sepal_width"], title="Dataset Iris - 3D (posicionamiento de la muestra)")

        # A√±adir el punto nuevo si hay valores
        try:
            if pred_button:
                fig3d.add_trace(go.Scatter3d(
                    x=[sep_len],
                    y=[pet_len],
                    z=[pet_wid],
                    mode='markers+text',
                    marker=dict(size=6, symbol='diamond', color='black'),
                    text=[f"Nuevo: {pred_label}"],
                    textposition="top center",
                    name="Nuevo"
                ))
        except Exception:
            pass

        st.plotly_chart(fig3d, use_container_width=True)

# ---------------------------
# Footer: informaci√≥n y descargas
# ---------------------------
st.markdown("---")
st.caption("App integrada ‚Äî Entrenamiento y dashboard en un solo archivo.")
st.write("Archivos relevantes (local):")
colf1, colf2 = st.columns(2)
with colf1:
    if os.path.exists(MODEL_PATH):
        st.write(f"- Modelo guardado: `{MODEL_PATH}`")
        with open(MODEL_PATH, "rb") as f:
            st.download_button("üì• Descargar modelo (.joblib)", data=f, file_name="model_iris_rf.joblib")
    else:
        st.write("- No hay modelo guardado (usa Entrenar / Actualizar).")
with colf2:
    if os.path.exists(loaded_path) and loaded_path != "uploaded":
        st.write(f"- Dataset local: `{loaded_path}`")
    else:
        st.write("- Dataset: cargado desde uploader o ruta temporal.")
    if os.path.exists(PDF_PATH):
        with open(PDF_PATH, "rb") as f:
            st.download_button("üì• Descargar enunciado (PDF)", data=f, file_name="IRIS_SPECIES_CLASSIFICATION_PROYECT.pdf")

# Fin del archivo